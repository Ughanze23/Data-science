{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ughanzepolycarp@gmail.compt2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kh5ihpoXwHkP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh5Jv0AouM0p",
        "colab_type": "text"
      },
      "source": [
        "# Challenge : Predicting Loan pre-delinquency using Deep learning classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-C1rnr9uWAy",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "in this challenge i will be working with Carbo'ns loan disbursement dataset to predict whether or not a loan will be defaulted on or paid back.The dataset contains details about clients, their location ,  income, employment status and other features used to determine whether a loan should be given or not. The goal of this challenge is  to build both a random forest classifier and a neural network model. Once the models have been trained and tested,i will then evaluate and compare them and explain which is best. in this notebook i will be building a deep learning Neural network to predict loan defaults\n",
        "This notebook is organised as follows :\n",
        "1. Data Upload\n",
        "2. Data preparation\n",
        "3. Building the models\n",
        "5. Model Evaluation and Comparision\n",
        "6. Conclusion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP5J0D8nuq9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import some packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGoQC26MuzEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upload data\n",
        "df = pd.read_csv('cleanData')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKduOUnWOaDB",
        "colab_type": "code",
        "outputId": "e7f7d7fb-65d7-43ac-8efa-8e5aa0b58145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(159589, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcnqwvx2u0vf",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data preparation\n",
        "in this section i will perform several operations to prepare my data for training. i will handle missing values, one-hot-encode categorical data, balance the classes, select relevant features i deem fit and split my data into a training and testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfi7OJLqvEIp",
        "colab_type": "text"
      },
      "source": [
        "The data has already been cleaned and missing values taken care of in the first notebook, so i will be skipping this action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD1y54yRvCDz",
        "colab_type": "text"
      },
      "source": [
        "### One-hot-encoding\n",
        "Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. so i will be performing One-hot-encoding on certain categorical features. before this i will select my X features and create my ouput Y(Label)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aai58zoKvT34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#select Features\n",
        "X = df[['clientIncome', 'incomeVerified', 'clientAge',\n",
        "       'clientGender', 'clientMaritalStatus', 'clientLoanPurpose',\n",
        "       'clientResidentialStatus', 'clientState', 'clientTimeAtEmployer',\n",
        "       'clientNumberPhoneContacts', 'clientAvgCallsPerDay','loanNumber','loanAmount',\n",
        "       'interestRate', 'loanTerm', 'max_amount_taken', 'max_tenor_taken','settleDays', 'firstPaymentRatio','firstPaymentDefault']]\n",
        "Y = df['loanDefault']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pxhCLYOKOUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#one hot encode categorical features\n",
        "X = pd.get_dummies(X,columns=['clientMaritalStatus','incomeVerified','clientResidentialStatus','clientGender','clientState','clientLoanPurpose'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxJc-KlZvZB1",
        "colab_type": "text"
      },
      "source": [
        "### Train and Test split\n",
        "i will Split the data into training set (70%), and test set (30%). Training set will be used to fit the model, and test set will be to evaluate the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-KGAvVmu7b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Skicit-learn to split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into training and testing sets\n",
        "X_train,Y_train,X_labels,Y_labels = train_test_split(X, Y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeLWMvsevifq",
        "colab_type": "text"
      },
      "source": [
        "### Balance the Classes\n",
        "Classification problems in most real world applications have imbalanced data sets. In other words, the positive examples (minority class) are a lot less than negative examples (majority class). in our dataset we have 72% (no default)negative values and 28% positive(loan default).class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class, that optimizes the model to make predictions based on the majority class in the dataset. so i am going  to balance the data set to achieve a model that is able to generalize and make good predictions on the minority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPCzecGgvozj",
        "colab_type": "text"
      },
      "source": [
        "## 3. Buliding The Models.\n",
        "The next step is to build the model. i will build a  neural network, that performs a binary classification on each loan. Also i will perform hyper parameter tuning on the model to improve the performance. The following Metrics, will be used to evaluate my final model.\n",
        "\n",
        "1.  **Accuracy** :\n",
        "Itâ€™s the ratio of the correctly labeled subjects to the whole pool of subjects.\n",
        "Accuracy is the most intuitive one.\n",
        "2. **Precision** :\n",
        "Precision is the ratio of the correctly +ve labeled by our program to all +ve labeled.\n",
        "3. **Recall ** :\n",
        "Recall is the ratio of the correctly +ve labeled by our program to all who are diabetic in reality.\n",
        "4. ** F1-score**\n",
        "F1 Score is the weighted average of precision and recall.\n",
        "5. ** Auc ** :If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a higher predicted probability to the positive observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBjM5LqNv_75",
        "colab_type": "text"
      },
      "source": [
        "**General Flow**:\n",
        "I will first build a base model, then do hyper parameter tuning, select the model with the best parameters. then use class balancing techiques  on the best model to build a model that predicts the minority class with higher accuracy(recall) i.e improving the Recall of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh5ihpoXwHkP",
        "colab_type": "text"
      },
      "source": [
        "### Deep learning model\n",
        "I will be buidling a NN classifer using Keras\n",
        "write about the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPq5EI6J0L0s",
        "colab_type": "code",
        "outputId": "9b25bfb1-dfef-4fe3-a09c-349f708a1c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#import required packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.utils import np_utils\n",
        "\n",
        "#import evaluation metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBNqngUw0bK8",
        "colab_type": "text"
      },
      "source": [
        "**Build base model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2j7qoHaGkGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create model\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL5XohU6HcOW",
        "colab_type": "code",
        "outputId": "499cc402-70ab-4ad3-c18c-5de8b015621f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Add an input layer \n",
        "model.add(Dense(512, activation='relu', input_dim=104))\n",
        "\n",
        "# Add one hidden layer \n",
        "model.add(Dense(8, activation='relu'))\n",
        "\n",
        "# Add an output layer \n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm_NrDemTHG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERabomywH3jO",
        "colab_type": "code",
        "outputId": "b61f1524-d008-45e8-8e00-de70e595b6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "                   \n",
        "#fit model\n",
        "base = model.fit(X_train, X_labels,epochs=2, batch_size=1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "111712/111712 [==============================] - 553s 5ms/step - loss: 8.4153 - acc: 0.4735\n",
            "Epoch 2/2\n",
            "111712/111712 [==============================] - 554s 5ms/step - loss: 4.4797 - acc: 0.7221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtR_2zxp2GOe",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To7a3vc52R94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict with model\n",
        "y_pred = model.predict(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz48iM_KZlQo",
        "colab_type": "code",
        "outputId": "3df5ef04-dfad-4b37-e64e-7e1f75556637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#accuracy\n",
        "accuracy_score(y_pred,Y_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7235206884307706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcHr7Q-OZ2RV",
        "colab_type": "code",
        "outputId": "89c44bdc-ed9c-450b-c3dd-7931e6f744bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#recall\n",
        "recall_score(Y_labels,y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8kSF_rMZ6Ky",
        "colab_type": "code",
        "outputId": "659d6c84-4196-4b17-dd26-3680e8b1066d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#Precision\n",
        "precision_score(Y_labels,y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75UPe0RAZ2K7",
        "colab_type": "code",
        "outputId": "35bbaf71-66f6-4dc3-f41e-fea127b0a5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#f1 score\n",
        "f1_score(Y_labels,y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfU1cOvtwLJM",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter tuning\n",
        "i will be using a third party hyperparameter optimization tool to do hyperparameter tuning of my deep learning model. Keras can be combined with Hyperopt to do hyper parameter tuning.\n",
        "Hyperas is  very simple convenience wrapper around hyperopt for fast prototyping with keras models. Hyperas lets you use the power of hyperopt without having to learn the syntax of it. Instead, just define your keras model as you are used to, but use a simple template notation to define hyper-parameter ranges to tune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8IxHi-QPJRd",
        "colab_type": "code",
        "outputId": "a53cbc3b-0c2c-4362-dd7f-3d662dc33c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1143
        }
      },
      "source": [
        "!pip install hyperas"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.4)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.4.1)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.4.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.3.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.4.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.4.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (6.0.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.2.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.16.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.5.0)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.28.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.2)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.7.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1->nbformat->hyperas) (4.4.0)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (5.2.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.4.2)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (5.5.0)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 337kB 27.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (2.5.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (40.9.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (4.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "\u001b[31mipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: hyperas, prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 1.0.15\n",
            "    Uninstalling prompt-toolkit-1.0.15:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.15\n",
            "Successfully installed hyperas-0.4.1 prompt-toolkit-2.0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na-EML8F1YRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "from hyperopt import Trials, STATUS_OK, tpe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSt4ZnOL2j7d",
        "colab_type": "text"
      },
      "source": [
        "i would like to reference the source of this code :https://github.com/maxpumperla/hyperas\n",
        ",without him this would not be possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3E-qT8bRI4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data():\n",
        "    '''\n",
        "    Data providing function:\n",
        "    This function is separated from model() so that hyperopt\n",
        "    won't reload data for each evaluation run.\n",
        "    '''\n",
        "    X_train,Y_train,X_labels,Y_labels = train_test_split(X, Y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhoI1Pzl2iiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyper parameter tunning \n",
        "def create_model(X_train, Y_train, X_labels, Y_labels):\n",
        "    \"\"\"\n",
        "    Model providing function:\n",
        "\n",
        "    Create Keras model with double curly brackets dropped-in as needed.\n",
        "    Return value has to be a valid python dictionary with two customary keys:\n",
        "        - loss: Specify a numeric evaluation metric to be minimized\n",
        "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
        "    The last one is optional, though recommended, namely:\n",
        "        - model: specify the model just created so that we can later use it again.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512,input_dim=104)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
        "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "\n",
        "    # If we choose 'four', add an additional fourth layer\n",
        "    if {{choice(['three', 'four'])}} == 'four':\n",
        "        model.add(Dense(100))\n",
        "\n",
        "        # We can also choose between complete sets of layers\n",
        "\n",
        "        model.add({{choice([Dropout(0.5), Activation('linear')])}})\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
        "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
        "\n",
        "    result = model.fit(X_train, X_labels,\n",
        "              batch_size={{choice([64, 128])}},\n",
        "              epochs=2,\n",
        "              verbose=2,\n",
        "              validation_split=0.1)\n",
        "    #get the highest validation accuracy of the training epochs\n",
        "    validation_acc = np.amax(result.history['val_acc']) \n",
        "    print('Best validation acc of epoch:', validation_acc)\n",
        "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    best_run, best_model = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=5,\n",
        "                                          trials=Trials())\n",
        "    X_train, Y_train, X_labels, Y_labels = data()\n",
        "    print(\"Evalutation of best performing model:\")\n",
        "    print(best_model.evaluate(Y_train, Y_labels))\n",
        "    print(\"Best performing model chosen hyper-parameters:\")\n",
        "    print(best_run)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgTBDnLYwXjU",
        "colab_type": "text"
      },
      "source": [
        "**Balance the classes and train on the best performing model**\n",
        "\n",
        "Oversampling and undersampling both have major drawbacks like prone to overfitting in oversampling minority class and loss of information by reducing the majority class.\n",
        "the technique i will be using to balance the data set are:\n",
        "\n",
        "* **Synthetic Minority Oversampling Technique (SMOTE):** It over-samples the minority class but using synthesized examples. It operates on feature space not the data space. \n",
        "i will be using alibrary - imbalanced learn to perform this operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuimQgiD8Od0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import RobustScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjpuHMZRaj5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#perform SMOTE)\n",
        "#first scale the data\n",
        "scalar = RobustScaler()\n",
        "X_train = scalar.fit_transform(X_train)\n",
        "Y_train = scalar.fit_transform(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVq4cFqv9GZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model with Smote\n",
        "smote = SMOTE(random_state=42,ratio='minority')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2AfLtRxaqt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#res means resampled\n",
        "X_train_res, X_labels_res = smote.fit_sample(X_train, X_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6DiLPwSausB",
        "colab_type": "code",
        "outputId": "e0d49011-2e1a-4653-c1d6-08bf549fb537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#train on resampled data\n",
        "model.fit(X_train_res, X_labels_res,epochs=3, batch_size=1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "161328/161328 [==============================] - 790s 5ms/step - loss: 0.3648 - acc: 0.8245\n",
            "Epoch 2/3\n",
            "161328/161328 [==============================] - 805s 5ms/step - loss: 0.3530 - acc: 0.8302\n",
            "Epoch 3/3\n",
            "161328/161328 [==============================] - 813s 5ms/step - loss: 0.3438 - acc: 0.8371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f609a8f6358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHyK0YxQrPRu",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate model performance using Metrics**\n",
        "* this is the performance of the model after balancing the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ptA_bdbTmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict test labels\n",
        "y_pred = model.predict(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrJZ0IhelF0N",
        "colab_type": "code",
        "outputId": "29a56a40-5a6d-4c7e-93dd-966d063bd1e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#recall\n",
        "recall_score(Y_labels,y_pred.round())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.740197930044572"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63rmgzu6mvIS",
        "colab_type": "code",
        "outputId": "f022721e-78ae-4538-9dd4-38389ea8bc05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Precision\n",
        "precision_score(Y_labels,y_pred.round())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7148172466622893"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkmJeDOgm3vu",
        "colab_type": "code",
        "outputId": "bfcca54c-c1be-44dd-82f1-f02bb2ecaaed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#f1 score\n",
        "f1_score(Y_labels,y_pred.round())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7272862232779097"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-dcPrG3pKMr",
        "colab_type": "code",
        "outputId": "491d1d20-2164-4b36-8b24-91c2c1c53163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#AUC\n",
        "roc_auc_score(y_pred.round(),Y_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8070867035184436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkSCY9kpnCSP",
        "colab_type": "code",
        "outputId": "d0217cb5-7877-4eb7-b2ef-3e4e60b4b80f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#accuracy\n",
        "accuracy_score(y_pred.round(),Y_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8465233828351818"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3ag-dK9wfu3",
        "colab_type": "text"
      },
      "source": [
        "## 4.  Model Evaluation and Comparision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfQEiqeVoZ1C",
        "colab_type": "text"
      },
      "source": [
        "Comparing the performance of models at training and testing.\n",
        "The business problem being to reduce the risk of Carbon losing money due to clients defaulting on loans, My focus was to build a model that was geared towards having a High recall(predicting positvie values as positive) and High AUC. After building the base model , i noticed that the recall,precision and Auc where all Zeros . i then balanced the data set using SMOTE technique to oversample the minority class. the final model had the highest** Precision : 0.71,** **Accuracy : 0.84**,\n",
        "**Recall : 0.74**, and an **AUC : of  0.81**. i Know further tuning could be performed on the model but that was not possible for me since, everytime i tried installing my hyperas library to perform hyperparameter tuning, my environment kept crashing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs-DZtmjnoX_",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX0xXxk6qOK2",
        "colab_type": "text"
      },
      "source": [
        "The NN classifer out performs the Random forest Classifer in every performance metric except the Auc. further tuning has not been performed but still i was able to get a Recall of 0.74 far better than the 0.64 i got from my Random forest classifier after tuning.with the focus on reducing loan defaults , my NN classifier is the go to model since it has a better recall than the Random forest Classifer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fynlE3Mhq_5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}